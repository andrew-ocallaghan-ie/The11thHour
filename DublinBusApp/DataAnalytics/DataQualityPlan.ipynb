{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Quality Plan\n",
    "Analysing the Dublin Bus Data provided to optimise model performance\n",
    "\n",
    "## Summary\n",
    "Each file has about 750,000 rows. Three quarters of a million.\n",
    "\n",
    "## Ideas to handle data.\n",
    "1. __Incremental Handing__\n",
    "__Route Division__\n",
    "Logic: Partition data into subsets of data which are internally comparable.\n",
    "         Does it make sense that a single model should describe all routes?\n",
    "         Since routes are different they should be modeled individually.\n",
    "\n",
    "A machine can maintain 12 DF max in a single object. (atleast mine can)\n",
    "If instead we extract each route from the data to build a dataframe of a single \n",
    "route. Then model it, take the best model. Now we have 100th of the data to handle in each increment.\n",
    "\n",
    "When we have derived the model, we store it and its route, but can discard the data then. \n",
    "\n",
    "If we create a function that returns the model. We can either use re-assignment or scoping discard the dataframe in python.\n",
    "\n",
    "For each route we store a model which is the sum of the models for the sections in which it is composed.\n",
    "\n",
    "__Section Division__\n",
    "Logic: The sum of the parts gives the value of the whole.\n",
    "\n",
    "Many smaller data sets. This models the travel time for different sections in which the busses pass through. This reduces redundancy in the analysis of bus routes (which overlap)\n",
    "\n",
    "Though since there are more sections than bus routes, there will be many more models to store, problem then becomes iterative. given t_0 the time we , we can calculate t_1 (the time we arrive in section1 after traversing seciont_0) then we apply the model for section 1 with t_1 as an input. (recirsive problem, easily described iteritavly)\n",
    "\n",
    "the total time is the sum of all the predicted times for each section.\n",
    "\n",
    "### Task\n",
    "Create a function that tests different models and chooses the best.\n",
    "Run this for each section storing each model. Then given a departure point and a destination we can derive the appropriate route and the sum of its sections.\n",
    "\n",
    "How to derive the route. How often do time stamps occur. Accuracy of estimation of length of a section depends on the frequancy in which a time stamp is put out. Assume a section takes 100.5 seconds to cross. if a stamp is put out every second and there are 100 time stamps in a journey we know that the time taken is between 100 and 101 (small percentage error). The best estimate that can be made is that that time taken is in between 100 and 102.\n",
    "\n",
    "If a time stamp occurs only every 50 seconds, then in a 100 second interval there could be 2 or 3 time stamps. i.e. we will predict it takes between 100 and 150.\n",
    "\n",
    "Max-Min <= x <= (n+1/n)*(Max-Min), where n is the number of timestamps for a specific journey id occuring within a section)\n",
    "\n",
    "\n",
    "\n",
    "##\n",
    "2. __\n",
    "\n",
    "\n",
    "We can use sparse data, take only every third entry for a given bus on a route. We then can fit the whole frame?\n",
    "\n",
    "\n",
    "Best approach may be to develop models route by route, then save sparse data for increasing performance (if nexesary?)\n",
    "\n",
    "sql query to return every 30th row, (or nth row) to reduce data if needed.\n",
    "SELECT t.id, t.key\n",
    "FROM\t\n",
    "(\n",
    "    SELECT id, key, ROW_NUMBER() OVER (ORDER BY key) AS rownum\n",
    "    FROM datatable\n",
    ") AS t\n",
    "WHERE t.rownum % 30 = 0    -- or % 40 etc\n",
    "ORDER BY t.key\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andy\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#import sections\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from patsy import dmatrices\n",
    "import matplotlib.patches as mpatches\n",
    "import statsmodels.formula.api as sm\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from statsmodels.formula.api import logit\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "import os\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This was the code used to merge route 4\n",
    "\n",
    "# Right now these functions are only built on one level, but they will be modularised and optimised by the time we build the MASTER function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #method 1, ends at 12 dataframes.\n",
    "\n",
    "# path=os.getcwd() + \"\\\\data\"\n",
    "\n",
    "# def merge_frames(path):\n",
    "#     column_names = [\"Timestamp\",\n",
    "#                     \"LineID\", \n",
    "#                     \"Direction\",\n",
    "#                     \"Journey_Pattern_ID\", \n",
    "#                     \"Timeframe\", \n",
    "#                     \"Vehicle_Journey_ID\", \n",
    "#                     \"Operator\", \n",
    "#                     \"Congestion\", \n",
    "#                     \"Lon\",\n",
    "#                     \"Lat\", \n",
    "#                     \"Delay\", \n",
    "#                     \"Block_ID\",\n",
    "#                     \"Vehicle_ID\",\n",
    "#                     \"Stop_ID\",\n",
    "#                     \"At_Stop\"]\n",
    "    \n",
    "#     #should help loop run a little faster.\n",
    "#     read = pd.read_csv\n",
    "#     concat = pd.concat\n",
    "    \n",
    "#     #stores contents of a folder as a list.\n",
    "#     contents = os.listdir(path) \n",
    "#     content_length = len(contents)\n",
    "    \n",
    "    \n",
    "#     #reads csv from data folder in cwd\n",
    "#     accumulator = read(path+\"\\\\\"+contents[0], index_col=None, header=0, encoding=\"utf-8\")\n",
    "#     accumulator.columns = column_names\n",
    "    \n",
    "#     for i in range(content_length):\n",
    "\n",
    "#         next_df = pd.read_csv(path+\"\\\\\"+contents[i], index_col=None, header=0)\n",
    "#         next_df.columns = column_names\n",
    "\n",
    "#         accumulator = pd.concat([accumulator[(accumulator[\"LineID\"]==4)], next_df[(next_df[\"LineID\"]==4)]], axis=0)\n",
    "#         print(accumulator.shape, \"acc\")\n",
    "        \n",
    "#     return accumulator\n",
    "\n",
    "# df_1 = merge_frames(path)\n",
    "\n",
    "# df_1.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This was the code used to get route data for all files by lineID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# path=os.getcwd() + \"\\\\data\"\n",
    "\n",
    "# def extract_merge_frames(path):\n",
    "#     column_names = [\"Timestamp\",\n",
    "#                     \"LineID\", \n",
    "#                     \"Direction\",\n",
    "#                     \"Journey_Pattern_ID\", \n",
    "#                     \"Timeframe\", \n",
    "#                     \"Vehicle_Journey_ID\", \n",
    "#                     \"Operator\", \n",
    "#                     \"Congestion\", \n",
    "#                     \"Lon\",\n",
    "#                     \"Lat\", \n",
    "#                     \"Delay\", \n",
    "#                     \"Block_ID\",\n",
    "#                     \"Vehicle_ID\",\n",
    "#                     \"Stop_ID\",\n",
    "#                     \"At_Stop\"]\n",
    "    \n",
    "    \n",
    "#     #stores contents of a folder as a list.\n",
    "#     contents = os.listdir(path) \n",
    "#     content_length = len(contents)\n",
    "    \n",
    "    \n",
    "#     #reads csv from data folder in cwd\n",
    "#     read = pd.read_csv\n",
    "#     accumulator = read(path+\"\\\\\"+contents[0], index_col=None, header=0)\n",
    "#     accumulator.columns = column_names\n",
    "#     set_of_routes = set(accumulator.LineID.unique())\n",
    "#     comparison_set = set_of_routes\n",
    "    \n",
    "#     #should help loop run a little faster.\n",
    "#     concat = pd.concat\n",
    "#     unite = set_of_routes.union\n",
    "    \n",
    "        \n",
    "#     #read through all files once and determine maximum set of routes\n",
    "    \n",
    "#     for i in range(1,content_length):\n",
    "#         next_df = pd.read_csv(path+\"\\\\\"+contents[i], index_col=None, header=0)\n",
    "#         next_df.columns = column_names\n",
    "#         next_df.columns = column_names\n",
    "#         next_set_of_routes = set(next_df.LineID.unique())\n",
    "#         set_of_routes = unite(next_set_of_routes)\n",
    "#         print(\"routes extracted from file:\", i)\n",
    "    \n",
    "#     print(\"total set of routes\\n\", set(set_of_routes)==set(comparison_set))\n",
    "#     print(comparison_set - set_of_routes)\n",
    "#     print(set_of_routes - comparison_set)\n",
    "    \n",
    "#     for route in set_of_routes: \n",
    "#         try:\n",
    "#             print(route)\n",
    "#             int(route) #if i get past this op, i can continue\n",
    "\n",
    "#             for i in range(1, content_length):\n",
    "#                 print(\"extracting route:\",route,\"\\tfrom file \",i)\n",
    "#                 next_df = pd.read_csv(path+\"\\\\\"+contents[i], index_col=None, header=0)\n",
    "#                 next_df.columns = column_names\n",
    "#                 accumulator = pd.concat([accumulator[(accumulator[\"LineID\"]==route)], next_df[(next_df[\"LineID\"]==route)]], axis=0)\n",
    "        \n",
    "#             accumulator.to_csv( \"route-\" + str(int(route)) + \"-raw1.csv\", encoding = \"utf-8\" )\n",
    "        \n",
    "#             print (\"route:\", route , \"complete\\n\")\n",
    "        \n",
    "#         except:\n",
    "#             print(\"You just got NaN-ed!\")\n",
    "        \n",
    "        \n",
    "            \n",
    "#     return accumulator #i return is so i can assess the dataframe after\n",
    "\n",
    "# df_1 = extract_merge_frames(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the code used on the November data, which has different quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # extra routes 32, 142, 111,114, 51, 116 , 118\n",
    "\n",
    "# path=os.getcwd() + \"\\\\by_day-Nov2012\"\n",
    "\n",
    "# def extract_merge_frames(path):\n",
    "#     column_names = [\"Timestamp\",\n",
    "#                     \"LineID\", \n",
    "#                     \"Direction\",\n",
    "#                     \"Journey_Pattern_ID\", \n",
    "#                     \"Timeframe\", \n",
    "#                     \"Vehicle_Journey_ID\", \n",
    "#                     \"Operator\", \n",
    "#                     \"Congestion\", \n",
    "#                     \"Lon\",\n",
    "#                     \"Lat\", \n",
    "#                     \"Delay\", \n",
    "#                     \"Block_ID\",\n",
    "#                     \"Vehicle_ID\",\n",
    "#                     \"Stop_ID\",\n",
    "#                     \"At_Stop\"]\n",
    "    \n",
    "    \n",
    "#     #stores contents of a folder as a list.\n",
    "#     contents = os.listdir(path) \n",
    "#     content_length = len(contents)\n",
    "    \n",
    "    \n",
    "#     #reads csv from data folder in cwd\n",
    "#     read = pd.read_csv\n",
    "    \n",
    "#     accumulator = read(path+\"\\\\\"+contents[0], index_col=None, header=0)\n",
    "#     accumulator.columns = column_names\n",
    "#     accumulator.LineID.astype(\"str\")\n",
    "    \n",
    "#     #init for first loop\n",
    "#     set_of_routes = set(accumulator.LineID.unique())\n",
    "#     comparison_set = set_of_routes\n",
    "    \n",
    "#     #should help loop run a little faster.\n",
    "#     concat = pd.concat\n",
    "#     unite = set_of_routes.union\n",
    "    \n",
    "        \n",
    "#     #read through all files once and determine maximum set of routes for partitioning\n",
    "\n",
    "    \n",
    "#     for i in range(1,content_length):\n",
    "#         next_df = pd.read_csv(path+\"\\\\\"+contents[i], index_col=None, header=0)\n",
    "#         next_df.columns = column_names\n",
    "#         next_df.LineID.astype(\"str\")\n",
    "#         next_set_of_routes = set(next_df.LineID.unique())\n",
    "#         set_of_routes = unite(next_set_of_routes)\n",
    "#         print(\"routes extracted from file:\", i)\n",
    "    \n",
    "#     print(comparison_set - set_of_routes)\n",
    "#     print(set_of_routes - comparison_set)\n",
    "#     set_of_routes = sorted(set_of_routes, key = str)\n",
    "#     print(len(set_of_routes), set_of_routes)\n",
    "    \n",
    "#     intset = {str(x) for x in set_of_routes if isinstance(x, int)}\n",
    "#     strset = {x for x in set_of_routes if isinstance(x, str)}\n",
    "#     set_of_routes = sorted(intset.union(strset), key=str)\n",
    "#     print(\"new\", set_of_routes)\n",
    "    \n",
    "#     for route in set_of_routes: \n",
    "#         accumulator = read(path+\"\\\\\"+contents[0], index_col=None, header=0)\n",
    "#         accumulator.columns = column_names\n",
    "#         accumulator.LineID.astype(\"str\")\n",
    "#         try:\n",
    "#             print(route)\n",
    "#             print(str(route))#if\n",
    "\n",
    "#             for i in range(1, content_length):\n",
    "#                 print(\"extracting route:\",route,\"\\tfrom file \",i)\n",
    "#                 next_df = pd.read_csv(path+\"\\\\\"+contents[i], index_col=None, header=0)\n",
    "#                 next_df.columns = column_names\n",
    "#                 next_df.LineID.astype(\"str\")\n",
    "                \n",
    "#                 accumulator = concat([accumulator[(accumulator[\"LineID\"]==route)], next_df[(next_df[\"LineID\"]==route)]], axis=0)\n",
    "        \n",
    "#             #used random to make sure some files dont overwrite eachoter...\n",
    "#             accumulator.to_csv( \"route-\" + route + \"-raw2.csv\", encoding = \"utf-8\" )\n",
    "        \n",
    "#             print (\"route:\", route , \"complete\\n\")\n",
    "        \n",
    "#         except:\n",
    "#             print(\"You just got NaN-ed!\")\n",
    "        \n",
    "        \n",
    "            \n",
    "#     return accumulator #i return is so i can assess the dataframe after\n",
    "\n",
    "\n",
    "# df_1 = extract_merge_frames(path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# This is where i plan to build the master function\n",
    "\n",
    "### Master function should be build using stepwise refinement, it needs to strong and tested very well, it will be expensive to run, so lets make sure it does it correctly.\n",
    "\n",
    "\n",
    "This function has to read in raw- original csv data, parition it into the appropriate route info files\n",
    "re-derive some column entries, drop duplicates, re-build or drop nulls and must do, save data in utf8 format etc.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "this is the function that will partition, clean and save new csv files for analysis.\n",
    "its based off the function above (obviously) but contains small snippets of psuedo code and instructions.\n",
    "Replace line ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # extra routes 32, 142, 111,114, 51, 116 , 118\n",
    "\n",
    "# path=os.getcwd() + \"\\\\by_day-Nov2012\"\n",
    "\n",
    "# def extract_merge_frames(path):\n",
    "#     column_names = [\"Timestamp\",\n",
    "#                     \"LineID\", \n",
    "#                     \"Direction\",\n",
    "#                     \"Journey_Pattern_ID\", \n",
    "#                     \"Timeframe\", \n",
    "#                     \"Vehicle_Journey_ID\", \n",
    "#                     \"Operator\", \n",
    "#                     \"Congestion\", \n",
    "#                     \"Lon\",\n",
    "#                     \"Lat\", \n",
    "#                     \"Delay\", \n",
    "#                     \"Block_ID\",\n",
    "#                     \"Vehicle_ID\",\n",
    "#                     \"Stop_ID\",\n",
    "#                     \"At_Stop\"]\n",
    "    \n",
    "    \n",
    "#     #stores contents of a folder as a list.\n",
    "#     contents = os.listdir(path) \n",
    "#     content_length = len(contents)\n",
    "    \n",
    "    \n",
    "#     #reads csv from data folder in cwd\n",
    "#     read = pd.read_csv\n",
    "#     accumulator = read(path+\"\\\\\"+contents[0], index_col=None, header=0)\n",
    "#     accumulator.columns = column_names\n",
    "#     accumulator.Journey_Pattern_ID.astype(\"str\")\n",
    "    \n",
    "    \n",
    "#     #init for first loop\n",
    "#     set_of_routes = set(accumulator.Journey_Pattern_ID.unique())\n",
    "#     comparison_set = set_of_routes\n",
    "    \n",
    "#     #should help loop run a little faster.\n",
    "#     concat = pd.concat\n",
    "#     unite = set_of_routes.union\n",
    "    \n",
    "        \n",
    "#     #read through all files once and determine maximum set of routes\n",
    "    \n",
    "#     for i in range(1,content_length):\n",
    "#         next_df = pd.read_csv(path+\"\\\\\"+contents[i], index_col=None, header=0)\n",
    "#         next_df.columns = column_names\n",
    "#         next_df.Journey_Pattern_ID.astype(\"str\")\n",
    "#         next_set_of_routes = set(next_df.Journey_Pattern_ID.unique())\n",
    "#         set_of_routes =  unite(next_set_of_routes)\n",
    "#         print(\"routes extracted from file:\", i)\n",
    "    \n",
    "#     #Decision do we break routes on their directionality\n",
    "    \n",
    "#     \"\"\" read journey patter and slice first 4 digits (5 bits if you want to include direction)\n",
    "#     while leading zeros: remove zeros\n",
    "#     put remaining bits back into set\"\"\"\n",
    "    \n",
    "    \n",
    "#     #Run set comparison for debugging\n",
    "#     print(comparison_set - set_of_routes)\n",
    "#     print(set_of_routes - comparison_set)\n",
    "#     set_of_routes = sorted(set_of_routes, key = str)\n",
    "#     print(len(set_of_routes), set_of_routes)\n",
    "    \n",
    "    \n",
    "#     intset = {str(x) for x in set_of_routes if isinstance(x, int)}\n",
    "#     strset = {x for x in set_of_routes if isinstance(x, str)}\n",
    "#     set_of_routes = sorted(intset.union(strset), key=str)\n",
    "#     print(\"new\", set_of_routes)\n",
    "    \n",
    "    \n",
    "#     for route in set_of_routes: \n",
    "#         accumulator = read(path+\"\\\\\"+contents[0], index_col=None, header=0)\n",
    "#         accumulator.columns = column_names\n",
    "#         accumulator.LineID.astype(\"str\")\n",
    "#         try:\n",
    "#             print(route)\n",
    "#             print(str(route))#if\n",
    "\n",
    "#             for i in range(1, content_length):\n",
    "#                 print(\"extracting route:\",route,\"\\tfrom file \",i)\n",
    "#                 next_df = pd.read_csv(path+\"\\\\\"+contents[i], index_col=None, header=0)\n",
    "#                 next_df.columns = column_names\n",
    "#                 next_df.LineID.astype(\"str\")\n",
    "                \n",
    "#                 accumulator = concat([accumulator[(accumulator[\"LineID\"]==route)], next_df[(next_df[\"LineID\"]==route)]], axis=0)\n",
    "        \n",
    "#             #used random to make sure some files dont overwrite eachoter...\n",
    "#             accumulator.to_csv( \"route-\" + route + \"-raw2.csv\", encoding = \"utf-8\" )\n",
    "        \n",
    "#             print (\"route:\", route , \"complete\\n\")\n",
    "        \n",
    "#         except:\n",
    "#             print(\"You just got NaN-ed!\")\n",
    "        \n",
    "        \n",
    "            \n",
    "#     return accumulator #i return is so i can assess the dataframe after\n",
    "\n",
    "\n",
    "# df_1 = extract_merge_frames(path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andy\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2717: DtypeWarning: Columns (14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>LineID</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Journey_Pattern_ID</th>\n",
       "      <th>Timeframe</th>\n",
       "      <th>Vehicle_Journey_ID</th>\n",
       "      <th>Operator</th>\n",
       "      <th>Congestion</th>\n",
       "      <th>Lon</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Delay</th>\n",
       "      <th>Block_ID</th>\n",
       "      <th>Vehicle_ID</th>\n",
       "      <th>Stop_ID</th>\n",
       "      <th>At_Stop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>1352160012000000</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>029A1001</td>\n",
       "      <td>2012-11-05</td>\n",
       "      <td>2639</td>\n",
       "      <td>CF</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.190650</td>\n",
       "      <td>53.359283</td>\n",
       "      <td>-259</td>\n",
       "      <td>29007</td>\n",
       "      <td>33238</td>\n",
       "      <td>613</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>1352160033000000</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>029A1001</td>\n",
       "      <td>2012-11-05</td>\n",
       "      <td>2639</td>\n",
       "      <td>CF</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.190617</td>\n",
       "      <td>53.359299</td>\n",
       "      <td>-259</td>\n",
       "      <td>29007</td>\n",
       "      <td>33238</td>\n",
       "      <td>613</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>1352160053000000</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>029A1001</td>\n",
       "      <td>2012-11-05</td>\n",
       "      <td>2639</td>\n",
       "      <td>CF</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.190550</td>\n",
       "      <td>53.359482</td>\n",
       "      <td>-259</td>\n",
       "      <td>29007</td>\n",
       "      <td>33238</td>\n",
       "      <td>613</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>1352160071000000</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>029A1001</td>\n",
       "      <td>2012-11-05</td>\n",
       "      <td>2639</td>\n",
       "      <td>CF</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.190550</td>\n",
       "      <td>53.359482</td>\n",
       "      <td>-259</td>\n",
       "      <td>29007</td>\n",
       "      <td>33238</td>\n",
       "      <td>613</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>1352160092000000</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>029A1001</td>\n",
       "      <td>2012-11-05</td>\n",
       "      <td>2639</td>\n",
       "      <td>CF</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.190283</td>\n",
       "      <td>53.359299</td>\n",
       "      <td>-259</td>\n",
       "      <td>29007</td>\n",
       "      <td>33238</td>\n",
       "      <td>613</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Timestamp  LineID  Direction Journey_Pattern_ID   Timeframe  \\\n",
       "122  1352160012000000      29          0           029A1001  2012-11-05   \n",
       "274  1352160033000000      29          0           029A1001  2012-11-05   \n",
       "459  1352160053000000      29          0           029A1001  2012-11-05   \n",
       "601  1352160071000000      29          0           029A1001  2012-11-05   \n",
       "743  1352160092000000      29          0           029A1001  2012-11-05   \n",
       "\n",
       "     Vehicle_Journey_ID Operator  Congestion       Lon        Lat  Delay  \\\n",
       "122                2639       CF           0 -6.190650  53.359283   -259   \n",
       "274                2639       CF           0 -6.190617  53.359299   -259   \n",
       "459                2639       CF           0 -6.190550  53.359482   -259   \n",
       "601                2639       CF           0 -6.190550  53.359482   -259   \n",
       "743                2639       CF           0 -6.190283  53.359299   -259   \n",
       "\n",
       "     Block_ID  Vehicle_ID Stop_ID  At_Stop  \n",
       "122     29007       33238     613        0  \n",
       "274     29007       33238     613        0  \n",
       "459     29007       33238     613        0  \n",
       "601     29007       33238     613        0  \n",
       "743     29007       33238     613        0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_1.Timestamp.astype(\"datetime64[ms]\")\n",
    "df_29A = pd.read_csv(\"route-29-raw2.csv\", index_col=0)\n",
    "df_29A.Journey_Pattern_ID.astype(\"str\")\n",
    "df_18 = pd.read_csv(\"route-18-raw2.csv\", index_col=0)\n",
    "# del df_29['Unnamed:0']\n",
    "# del df_29A['Unnamed:0']\n",
    "df_29A.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df_18.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['029A1001' 'null' '029A0001' '00180001' nan]\n",
      "['00181001' '00180001' 'null' '00181002' nan '01451001']\n"
     ]
    }
   ],
   "source": [
    "print(df_29A.Journey_Pattern_ID.unique())\n",
    "print(df_18.Journey_Pattern_ID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'738', '741', '4804', 5154, 5157, '742', '658', '745', '617', '5157', '609', 600, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, '950', 621, 620, '616', '605', '655', '604', '659', '608', '618', 654, 655, 656, 657, 658, '611', 659, 675, '291', '612', 4800, 4804, '7551', '915', '739', '744', 738, 739, '740', 741, 742, 743, 744, 745, '613', 740, '615', '621', 291, 296, '606', '620', '657', '5154', '916', '619', '600', '656', '525', '614', 7551, '610', '918', 915, 916, 917, 918, 919, '917', '4800', 950, '296', '919', '607', '743', '675', '654'}\n"
     ]
    }
   ],
   "source": [
    "# open dataframe of 31 data. view unique bus stops where journey_parrern_id equal 31.\n",
    "#check that every journey pattern id in that file has that sequence of bus stops\n",
    "\n",
    "#check that every 32000xxx has a different sequence.\n",
    "\n",
    "stops_29 = set(df_29A[df_29A[\"Journey_Pattern_ID\"]==\"029A1001\"].Stop_ID.unique())\n",
    "print(stops_29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stops_18 = set(df_18[df_18[\"Journey_Pattern_ID\"]==\"00181001\"].Stop_ID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "{'7526', '2781', '2655', '2482', '2331', '753', '850', '486', '2479', '2487', '5026', '7529', '2206', '2208', '2727', '2726', '2793', '1077', '2211', '7530', '2212', '487', '2808', '2105', '7525', '2656', '2672', '2480', '2485', '2101', '2448', '385', '2653', '2654', '2782', '2330', '2798', '851', '2489', '2780', '2481', '2484', '2807', '2806', '4888', '754', '2791', '4357', '7527', '2488', '7165', '2207', '2799', '2783', '2102', '2204', '2794', '2210', '849', '4359', '755', '2486', '2668', '2673', '2652', '2328', '2483', '2696', '2688', '2329', '2689', '2450', '7528'}\n",
      "{'738', '741', '4804', 5154, 5157, '742', '658', '745', '617', '5157', '609', 600, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, '950', 621, 620, '616', '605', '655', '604', '659', '608', '618', 654, 655, 656, 657, 658, '611', 659, 675, '291', '612', 4800, 4804, '7551', '915', '739', '744', 738, 739, 740, 741, '740', '613', 742, 743, 744, 745, '615', '621', 291, 296, '606', '620', '657', '5154', '916', '619', '600', '656', '525', '614', 7551, '610', '918', 915, 916, 917, 918, 919, '917', '4800', 950, '296', '919', '607', '743', '675', '654'}\n",
      "set()\n",
      "True\n",
      "\n",
      "\n",
      "False\n",
      "{654, 655, 656, 657, 658, 659, 916, 917, 918, 919, 742, 5154, 675, 5157, 296, 4800, 4804, 600, 604, 605, 606, 607, 608, 609, 610, 739, 740, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 743, 744, 745, 7551}\n",
      "{7552, 515, 516, 519, 521, 522, 523, 649, 653, 526, 527, 528, 529, 530, 531, 532, 924, 925, 927, 4384, 4386, 5155, 5156, 5160, 938, 709, 4806, 650, 651, 722, 723, 724, 725, 726, 727, 728, 729, 730, 496, 497, 4472}\n",
      "\n",
      "{738, 291, 741, 525, 915, 950}\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# I use this code to prove using signitures that journey pattern ids contain more accurate route info that Line_id\n",
    "\n",
    "\n",
    "print(stops_18 == stops_29)\n",
    "print(stops_18 - stops_29)\n",
    "print(stops_29 - stops_18)\n",
    "print(stops_29.intersection(stops_18))\n",
    "print(len(stops_29.union(stops_18))== len(stops_29) + len(stops_18))\n",
    "\n",
    "stops_29A1 = set(df_29A[df_29A[\"Journey_Pattern_ID\"]==\"029A1001\"].Stop_ID.unique())\n",
    "\n",
    "intset = {x for x in stops_29A1 if isinstance(x, int)}\n",
    "strset = {int(x) for x in stops_29A1 if isinstance(x, str)}\n",
    "stops_29A1 = set(sorted(intset.union(strset), key=str))\n",
    "\n",
    "stops_29A2 = set(df_29A[df_29A[\"Journey_Pattern_ID\"]==\"029A0001\"].Stop_ID.unique())\n",
    "intset = {x for x in stops_29A2 if isinstance(x, int)}\n",
    "strset = {int(x) for x in stops_29A2 if isinstance(x, str)}\n",
    "stops_29A2 = set(sorted(intset.union(strset), key=str))\n",
    "\n",
    "\n",
    "# print((stops_29A1))\n",
    "# print(\"\")\n",
    "# print((stops_29A2))\n",
    "# print(\"\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(stops_29A1 == stops_29A2)\n",
    "print(stops_29A1 - stops_29A2)\n",
    "print(stops_29A2 - stops_29A1)\n",
    "print()\n",
    "print(stops_29A2.intersection(stops_29A1))\n",
    "print(len(stops_29A2.union(stops_29A1)) == len(stops_29A2) + len(stops_29A1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>LineID</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Journey_Pattern_ID</th>\n",
       "      <th>Timeframe</th>\n",
       "      <th>Vehicle_Journey_ID</th>\n",
       "      <th>Operator</th>\n",
       "      <th>Congestion</th>\n",
       "      <th>Lon</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Delay</th>\n",
       "      <th>Block_ID</th>\n",
       "      <th>Vehicle_ID</th>\n",
       "      <th>Stop_ID</th>\n",
       "      <th>At_Stop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1352160002000000</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>null</td>\n",
       "      <td>2012-11-05</td>\n",
       "      <td>2757</td>\n",
       "      <td>CF</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.246833</td>\n",
       "      <td>53.354599</td>\n",
       "      <td>775</td>\n",
       "      <td>32007</td>\n",
       "      <td>38083</td>\n",
       "      <td>3586</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1352160008000000</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>00320001</td>\n",
       "      <td>2012-11-05</td>\n",
       "      <td>2721</td>\n",
       "      <td>CF</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.124481</td>\n",
       "      <td>53.433102</td>\n",
       "      <td>65</td>\n",
       "      <td>32003</td>\n",
       "      <td>38079</td>\n",
       "      <td>3604</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>1352160023000000</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>null</td>\n",
       "      <td>2012-11-05</td>\n",
       "      <td>2757</td>\n",
       "      <td>CF</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.245217</td>\n",
       "      <td>53.355850</td>\n",
       "      <td>775</td>\n",
       "      <td>32007</td>\n",
       "      <td>38083</td>\n",
       "      <td>3586</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>1352160027000000</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>00320001</td>\n",
       "      <td>2012-11-05</td>\n",
       "      <td>2721</td>\n",
       "      <td>CF</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.123217</td>\n",
       "      <td>53.435215</td>\n",
       "      <td>65</td>\n",
       "      <td>32003</td>\n",
       "      <td>38079</td>\n",
       "      <td>3604</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>1352160041000000</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>null</td>\n",
       "      <td>2012-11-05</td>\n",
       "      <td>2757</td>\n",
       "      <td>CF</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.242033</td>\n",
       "      <td>53.358032</td>\n",
       "      <td>775</td>\n",
       "      <td>32007</td>\n",
       "      <td>38083</td>\n",
       "      <td>3586</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Timestamp  LineID  Direction Journey_Pattern_ID   Timeframe  \\\n",
       "13   1352160002000000      32          0               null  2012-11-05   \n",
       "78   1352160008000000      32          0           00320001  2012-11-05   \n",
       "162  1352160023000000      32          0               null  2012-11-05   \n",
       "220  1352160027000000      32          0           00320001  2012-11-05   \n",
       "351  1352160041000000      32          0               null  2012-11-05   \n",
       "\n",
       "     Vehicle_Journey_ID Operator  Congestion       Lon        Lat  Delay  \\\n",
       "13                 2757       CF           0 -6.246833  53.354599    775   \n",
       "78                 2721       CF           0 -6.124481  53.433102     65   \n",
       "162                2757       CF           0 -6.245217  53.355850    775   \n",
       "220                2721       CF           0 -6.123217  53.435215     65   \n",
       "351                2757       CF           0 -6.242033  53.358032    775   \n",
       "\n",
       "     Block_ID  Vehicle_ID Stop_ID  At_Stop  \n",
       "13      32007       38083    3586        0  \n",
       "78      32003       38079    3604        0  \n",
       "162     32007       38083    3586        0  \n",
       "220     32003       38079    3604        0  \n",
       "351     32007       38083    3586        0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_1.Timestamp.astype(\"datetime64[ms]\")\n",
    "df_31 = pd.read_csv(\"route-32-raw2.csv\", index_col=0)\n",
    "# del df_29['Unnamed:0']\n",
    "# del df_29A['Unnamed:0']\n",
    "df_31.head()\n",
    "#if its the same vehicle between in same journey_id then extrapolate the journey pattern id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['null' '00320001' '00321001' '032B1001' '032A1001' '032B0001' '032A0001'\n",
      " '032X0001' '032B0002' nan '032X1001' '032X1002' '032B1002' '00311001'\n",
      " '031A1001']\n"
     ]
    }
   ],
   "source": [
    "print(df_31.Journey_Pattern_ID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['289']\n",
      "\n",
      "[]\n",
      "\n",
      "False\n",
      "{'289'}\n",
      "set()\n",
      "set()\n",
      "True\n",
      "Timestamp             0\n",
      "LineID                0\n",
      "Direction             0\n",
      "Journey_Pattern_ID    0\n",
      "Timeframe             0\n",
      "Vehicle_Journey_ID    0\n",
      "Operator              0\n",
      "Congestion            0\n",
      "Lon                   0\n",
      "Lat                   0\n",
      "Delay                 0\n",
      "Block_ID              0\n",
      "Vehicle_ID            0\n",
      "Stop_ID               0\n",
      "At_Stop               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "stops_31A1 = set(df_31[df_31[\"Journey_Pattern_ID\"]==\"00311001\"].Stop_ID.unique())\n",
    "stops_31A2 = set(df_31[df_31[\"Journey_Pattern_ID\"]==\"00310002\"].Stop_ID.unique())\n",
    "print(sorted(stops_31A1))\n",
    "print(\"\")\n",
    "print(sorted(stops_31A2))\n",
    "print(\"\")\n",
    "print(stops_31A1 == stops_31A2)\n",
    "\n",
    "print(stops_31A1 - stops_31A2)\n",
    "print(stops_31A2 - stops_31A1)\n",
    "print(stops_31A2.intersection(stops_31A1))\n",
    "print(len(stops_31A2.union(stops_31A1)) == len(stops_31A2) + len(stops_31A1))\n",
    "print(df_31[df_31[\"Journey_Pattern_ID\"]==\"00310002\"].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to me that each journey pattern id is a different route\n",
    "/slightly modified route.\n",
    "\n",
    "line ID is somewhat unreliable (but convinient) \n",
    "one serve one direction the other serves another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-ff939f79e26c>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-ff939f79e26c>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    vehicle = df_31.loc(df_31.Vehicle_Journey_ID==2757)&(df.Timeframe=\"2012-11-05\")\u001b[0m\n\u001b[0m                                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "vehicle = df_31.loc(df_31.Vehicle_Journey_ID==2757)&(df.Timeframe=\"2012-11-05\")\n",
    "vehicle.sort_values(\"Timestamp\")\n",
    "vehicle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cleaning function : Needs to iterate through each file, accumulate rows for speccific routes and clean them and save them as a new csv.\n",
    "\n",
    "cleaning includes fixing Line_ID\n",
    "fixing Direction\n",
    "Dropping nulls\n",
    "feeds in weather data and attaches the classification columns for dummy variables.\n",
    "\n",
    "They are then saved again.\n",
    "\n",
    "we use \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114489, 15)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_31.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp             0\n",
       "LineID                0\n",
       "Direction             0\n",
       "Journey_Pattern_ID    2\n",
       "Timeframe             0\n",
       "Vehicle_Journey_ID    0\n",
       "Operator              0\n",
       "Congestion            0\n",
       "Lon                   0\n",
       "Lat                   0\n",
       "Delay                 0\n",
       "Block_ID              0\n",
       "Vehicle_ID            0\n",
       "Stop_ID               0\n",
       "At_Stop               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_31.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp             30876\n",
       "LineID                30876\n",
       "Direction             30876\n",
       "Journey_Pattern_ID    30876\n",
       "Timeframe             30876\n",
       "Vehicle_Journey_ID    30876\n",
       "Operator              30876\n",
       "Congestion            30876\n",
       "Lon                   30876\n",
       "Lat                   30876\n",
       "Delay                 30876\n",
       "Block_ID              30876\n",
       "Vehicle_ID            30876\n",
       "Stop_ID               30876\n",
       "At_Stop               30876\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_31[df_31[\"Journey_Pattern_ID\"]==\"null\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#alot of these nulls are -re-derivable if we use the signature method"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
