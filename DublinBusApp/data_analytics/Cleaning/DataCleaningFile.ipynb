{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning Plan\n",
    "Analysing the Dublin Bus Data provided to optimise model performance\n",
    "\n",
    "## Summary\n",
    "Each file has about 750,000 rows. Three quarters of a million. The data has been analysed extensivly. The purpose of this document is to analyses the raw data and its weaknesses and then try ogranise it in a useable way and synthesise it to be suitable for modeling.\n",
    "\n",
    "\n",
    "## Ideas to handle data.\n",
    "\n",
    "### __Incremental Handing__:\n",
    "\n",
    "\n",
    "Each file is big, handling multiple files and multiple dataframes which have the same scale as the files is expensive on memory.\n",
    "The files are currently supplied as the global dublin bus data for a given day. For handling, it would be preferable instead of having a csv file representing the full fleet for a day, if we could look at a single route for the month. (partition based on route pattern instead of day)\n",
    "\n",
    "Having data partitioned by routes makes it more internally consistent. Rows and sets of rows are more comparable with eachother as they pass the same sets of stops. Different routes operates relatively independently so we can split these into sizeable yet relatively independent parts. This structure is optimal since we wish to analyse and model each route individually.\n",
    "\n",
    "This process should be completed iteritively. Iterate through each file to extract every possible journey pattern.\n",
    "Then for each journey pattern, iterate through each file extracting, re-constructing infromation only for that journey pattern. If a programmer attempts all these tasks the dataframes rapidly use the available memory, as the meachine begins to run low on RAM and it has to access the disk (evan an ssd) performance decreases substantially. Also the process could fail due to a memory error.\n",
    "\n",
    "\n",
    "### __Re-Construction__:\n",
    "\n",
    "\n",
    "From analysing the data, we can see that many columns are incorrect or null. But by knowing the relationships and semantics between different columns it is possible to reconstruct \"lost\" infromation with reasonable levels of certainty. In the handling of this data. Journey_Pattern_ID will be used to re-construct LineID and Direction and VehicleID, Timeframe and Vehicle_Journey_ID, will be used to reconstruct missing Journey_Pattern_ID infromations.\n",
    "\n",
    "\n",
    "### __Analysis and Test Cases__:\n",
    "\n",
    "\n",
    "Other sections of this document were used to just explore the data and tease out properties of different columns to help the programmer gain understanding of the datas' structure and properties. These include certain observations about missing data, inconsistent data types and some mother anomalies.\n",
    "\n",
    "Other considerations and challenges form this data are - How to derive the route.\n",
    "How often do time stamps occur.\n",
    "\n",
    "\n",
    "\n",
    "### __Section Division__: (A potential analytic model for the data supplied)\n",
    "\n",
    "\n",
    "Logic: The sum of the parts gives the value of the whole.\n",
    "\n",
    "    - Many smaller data sets. This models the travel time for different sections in which the busses pass through.\n",
    "        - This reduces redundancy in the analysis of bus routes (which overlap)\n",
    "\n",
    "    - Though since there are more sections than bus routes, there will be many more models to store. \n",
    "        - Given t_0 the time we can calculate t_1 (the time we arrive in section1 after traversing seciont_0)\n",
    "            - Then we apply the model for section 1 with t_1 as an input. (recirsive problem, easily described iteritavly)\n",
    "                - The total time is the sum of all the predicted times for each section.\n",
    "\n",
    "    - Accuracy of estimation of length of a section depends on the frequancy in which a time stamp is put out.\n",
    "        - Assume a section takes 100.5 seconds to cross. \n",
    "            - if a stamp is put out every second and there are 100 time stamps in a journey.\n",
    "                - We know that the time taken is between 100 and 101 (small percentage error).\n",
    "                    - The best estimate that can be made is that that time taken is in between 100 and 102.\n",
    "\n",
    "    - If a time stamp occurs only every 50 seconds.\n",
    "        - Then in a 100 second interval there could be 2 or 3 time stamps.\n",
    "            - i.e. we will predict it takes between 100 and 150.\n",
    "    \n",
    "    - In general: Max-Min <= x <= (n+1/n)*(Max-Min) \n",
    "        - where n is the number of timestamps for a specific journey id occuring within a section)\n",
    "            - x is the actual time taken to cross the section\n",
    "                - Maximum margine of error is 1/n\n",
    "\n",
    "### __General Goal__:\n",
    "\n",
    "\n",
    "Create a function that tests different models on a route (or a section) or whatever scope the model has.\n",
    "Run this for each section/route (whatever scope is used) storing each model. Then given a departure point and a destination we can derive the appropriate route and the sum of its sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import sections something to cover all bases\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from patsy import dmatrices\n",
    "import matplotlib.patches as mpatches\n",
    "import statsmodels.formula.api as sm\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from statsmodels.formula.api import logit\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import time\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import threading\n",
    "\n",
    "import mp1_functions\n",
    "import mp2_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Processing Overview.\n",
    " \n",
    "Re-construction: First fixes journey pattern id (takes time), Secondly fixes LineID and Direction (very fast).\n",
    "                 Issues. For november data, there may be data typeing issues, these should be resolved here.\n",
    "\n",
    "Extraction: Partitions Data by LineID\n",
    "            Issues. Columns types when data is read in\n",
    "\n",
    "\n",
    "If I had to do this manually, what would I do?\n",
    "I would open a raw data time, I'd perform the re-constructions on each file, so I wouldn't have to do it again.\n",
    "     - Est Time 1 min re-construction 2 hours for extraction\n",
    "    \n",
    "     - Improvements: time-profile this function in eclipse (no improvement yeilded, try horizontal scaling on ec2)\n",
    "     - Possibly distribute its task accross a number of t2.micro instances to reduce the overall runtimes.\n",
    "     \n",
    "I would iterate over the re-constructed files extracting info route by route, then deleteing if from the sourece file so I wouldn't have to read it again. (iterations should speed up towards the end)\n",
    "Files will be saved to a seperate folder.\n",
    "\n",
    "All lineID and Pattern ID (they can contain numbers and letters) should be converted to the string type of themselves.\n",
    "\n",
    "note: I have cut code that does not fit the purpose of this document, the code demonstrating journey pattern id characteristics can be found in the previous commit. This document also comments out test cases so the reader can run them selevtively and restart the kernel at their convinience.\n",
    "\n",
    "Logic: Perform size reducing operations first such as at_stop\n",
    "\n",
    "Outline:\n",
    "\n",
    "    - Create a backup of the raw folder, that way we can modify a copied set of files directly.\n",
    "    - First Pass on data\n",
    "            - Drop all at stop == 0 in first pass\n",
    "            - Run re-constructions (direction and missing JPIDs)\n",
    "            - Drop Duplicates\n",
    "            - save (overwrite copy, not original)\n",
    "    - Extraction and modification\n",
    "            - Extract by LineID\n",
    "                - when looping through files, remove processed data (decreases order but increases co-efficients)\n",
    "                - Derive new columns (is weekend, holiday etc...)\n",
    "                - Merge weather\n",
    "                - Merges distances (avoid for now)\n",
    "                - other..?\n",
    "            - Save\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Construction 1\n",
    "This function basically teases out the most exclusive set of combinations of Timeframes, Vehicle Journey Ids and Vehicle Ids that have a \"null\" string for Vehicle Journey Pattern. (hopefully these conditions narrow down to series of data that represent a single journey) then it looks for any Journey Pattern ID that was transmitted during that journey and replaces all the null values with the known journey pattern.\n",
    "\n",
    "Limitation, re-constructing for a single route in a single file is comparably as expensive as constructing all routes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#could be optimised further: instead of allocating each thread and arrbitrary case load \n",
    "#(one might finish early and the other late, unlikely though)\n",
    "#use a queue , so each thread takes the next in the queue, giving them an even workload.\n",
    "           \n",
    "def fix_JPID(vjids, day, dataframe):\n",
    "    \n",
    "    #should help loop run a little faster\n",
    "    loc = dataframe.loc\n",
    "    total = len(vjids)\n",
    "    current = 0\n",
    "    for run in vjids: #vehicle_journey_id\n",
    "        current+=100\n",
    "        print(current//total,\"% Reconstructed for \", day)\n",
    "        #vehicle ids\n",
    "        vids = set(dataframe[ (dataframe    [ \"Timeframe\" ]     ==  day   ) &\\\n",
    "                          (dataframe [\"Journey_Pattern_ID\"] == \"null\" ) &\\\n",
    "                          (dataframe [\"Vehicle_Journey_ID\"] ==  run   ) ]\\\n",
    "                          .Vehicle_ID.unique())\n",
    "\n",
    "        for vehicle in vids:\n",
    "            #list of potential journeys eliguble for re-construction\n",
    "            re_construct = list(loc[ (dataframe    [\"Timeframe\"]       ==   day )  &\\\n",
    "                                     (dataframe [\"Vehicle_Journey_ID\"] ==   run )  &\\\n",
    "                                     (dataframe    [\"Vehicle_ID\"]      == vehicle ),\\\n",
    "                                     \"Journey_Pattern_ID\" ].unique() )  \n",
    "\n",
    "\n",
    "            #re-constructs journey pattern to non-null entry\n",
    "            if len(re_construct) == 2:\n",
    "                if re_construct[0] != \"null\":\n",
    "                    #replaces nulls\n",
    "                    loc[ (dataframe    [\"Timeframe\"]       ==  day    ) &\\\n",
    "                         (dataframe [\"Vehicle_Journey_ID\"] ==  run    ) &\\\n",
    "                         (dataframe    [\"Vehicle_ID\"]      == vehicle ), \\\n",
    "                         \"Journey_Pattern_ID\" ] = re_construct[0]\n",
    "\n",
    "                else:\n",
    "                    #replaces nulls\n",
    "                    loc[ (dataframe    [\"Timeframe\"]        == day     ) &\\\n",
    "                         (dataframe [\"Vehicle_Journey_ID\"]  == run     ) &\\\n",
    "                         (dataframe    [\"Vehicle_ID\"]       == vehicle ), \\\n",
    "                         \"Journey_Pattern_ID\" ] = re_construct[1]\n",
    "                    \n",
    "    return dataframe\n",
    "                \n",
    "\n",
    "#This wraps the fixing function to run it on seperate threads\n",
    "def wrap_JPID(dataframe):\n",
    "    #every day\n",
    "    time_frames = set(dataframe[ dataframe[\"Journey_Pattern_ID\"] == \"null\" ].Timeframe.unique())\n",
    "    total1=len(time_frames)\n",
    "    \n",
    "#     for raw data this loop is redundant but negligable. its here for extensability of purpose.\n",
    "    for day in time_frames:\n",
    "        \n",
    "    #find list of vehicle journey ids with nulls during that day\n",
    "        vjid = list( dataframe[ (dataframe   [ \"Timeframe\" ]      ==  day  ) &\\\n",
    "                                (dataframe [\"Journey_Pattern_ID\"] == \"null\") ]\\\n",
    "                                .Vehicle_Journey_ID.unique())                     \n",
    "        \n",
    "    return fix_JPID(vjid, day, dataframe)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test_me = pd.read_csv(\"siri.20130122.csv\")\n",
    "# this could take about an hour\n",
    "\n",
    "# columns   =    [\"Timestamp\",\n",
    "#                 \"LineID\", \n",
    "#                 \"Direction\",\n",
    "#                 \"Journey_Pattern_ID\", \n",
    "#                 \"Timeframe\", \n",
    "#                 \"Vehicle_Journey_ID\", \n",
    "#                 \"Operator\", \n",
    "#                 \"Congestion\", \n",
    "#                 \"Lon\",\n",
    "#                 \"Lat\", \n",
    "#                 \"Delay\", \n",
    "#                 \"Block_ID\",\n",
    "#                 \"Vehicle_ID\",\n",
    "#                 \"Stop_ID\",\n",
    "#                 \"At_Stop\"]\n",
    "\n",
    "# test_me.columns = columns\n",
    "# before =\"__Before:__\\n\" + str(test_me[test_me[\"Journey_Pattern_ID\"] == \"null\"].count())\n",
    "# test_me = fix_JPID(test_me)\n",
    "# print(before)\n",
    "# print(\"__After:__\\n\",test_me[test_me[\"Journey_Pattern_ID\"] == \"null\"].count())\n",
    "# # this one takes unbelieveably long :(\n",
    "# it has to go from 0 to 100 once (its only one day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test_me = pd.read_csv(\"route-4-raw1.csv\", index_col=0)\n",
    "# # print(test_me.head())\n",
    "\n",
    "# columns   =    [\"Timestamp\",\n",
    "#                 \"LineID\", \n",
    "#                 \"Direction\",\n",
    "#                 \"Journey_Pattern_ID\", \n",
    "#                 \"Timeframe\", \n",
    "#                 \"Vehicle_Journey_ID\", \n",
    "#                 \"Operator\", \n",
    "#                 \"Congestion\", \n",
    "#                 \"Lon\",\n",
    "#                 \"Lat\", \n",
    "#                 \"Delay\", \n",
    "#                 \"Block_ID\",\n",
    "#                 \"Vehicle_ID\",\n",
    "#                 \"Stop_ID\",\n",
    "#                 \"At_Stop\"]\n",
    "\n",
    "# test_me.columns = columns\n",
    "# print(\"__Before:__\\n\",test_me[test_me[\"Journey_Pattern_ID\"] == \"null\"].count())\n",
    "# test_me = fix_JPID(test_me)\n",
    "# print(\"__After:__\\n\",test_me[test_me[\"Journey_Pattern_ID\"] == \"null\"].count())\n",
    "\n",
    "# test_me.to_csv(\"re_constructed_Dec\\\\route4-reconstructed.csv\", index = False)\n",
    "# # this one runs faster. it goes from 0 to 100 30 times (its a month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Construction 2. \n",
    "A simple section that re-derives LineID and Direction from the string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fix_LID_and_Dir(dataframe):\n",
    "    modified_frame = dataframe\n",
    "    modified_frame[\"Journey_Pattern_ID\"] = dataframe[\"Journey_Pattern_ID\"].astype(\"str\")\n",
    "    modified_frame[\"LineID\"] =  dataframe[\"Journey_Pattern_ID\"].str[:4]\n",
    "#     modified_frame[modified_frame[\"LineID\"].str[:3] == \"000\"] =  dataframe[\"Journey_Pattern_ID\"].str[3]\n",
    "#     modified_frame[modified_frame[\"LineID\"].str[:2] == \"00\"] =  dataframe[\"Journey_Pattern_ID\"].str[2:4]\n",
    "#     modified_frame[modified_frame[\"LineID\"].str[:1] == \"0\"] =  dataframe[\"Journey_Pattern_ID\"].str[1:4]\n",
    "    modified_frame[\"Direction\"] = dataframe[\"Journey_Pattern_ID\"].str[4]\n",
    "    \n",
    "    return modified_frame\n",
    "\n",
    "def drop_columns(dataframe):\n",
    "    modified_frame = dataframe\n",
    "    modified_frame = dataframe.drop(\"Congestion\", axis=1)\n",
    "    modified_frame = dataframe.drop(\"Delay\", axis=1)\n",
    "    modified_frame = dataframe.drop(\"Block_ID\", axis=1)\n",
    "    modified_frame = dataframe.drop(\"Operator\", axis=1)\n",
    "    \n",
    "    Congestion, operator, delay, block_id\n",
    "\n",
    "def drop_it_like_its_stop(dataframe):\n",
    "    modified_frame = dataframe.loc[(dataframe.At_Stop == 1)]\n",
    "    return modified_frame\n",
    "\n",
    "def drop_null_JPIDS(dataframe):\n",
    "    return dataframe[dataframe[\"Journey_Pattern_ID\"] != \"null\" ]\n",
    "\n",
    "def remove_idle_at_stop(df):\n",
    "    df = df.drop_duplicates(subset='Stop_ID', keep='first')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def route_order(df):\n",
    "    for i in range(df['Timestamp'].size):\n",
    "        df['stop_order'].values[i] = i+1    \n",
    "    return df\n",
    "\n",
    "def stops_made(dataframe):\n",
    "    modified_frame = dataframe\n",
    "    modified_frame[\"Stops_Made\"] = 0\n",
    "    modified_frame = modified_frame.groupby(['Vehicle_Journey_ID', 'Timeframe'])\n",
    "    modified_frame = remove_idle_at_stop(modified_frame)\n",
    "    modified_frame = modified_frame.groupby(['Vehicle_Journey_ID', 'Timeframe'])\n",
    "    modified_frame = route_order(modified_frame)\n",
    "    \n",
    "    return modified_frame    \n",
    "\n",
    "\n",
    "def drop_middle(dataframe):\n",
    "    \n",
    "    stops = pd.read_csv(\"end_stops.csv\")\n",
    "    col1 = stops.First_1.unique()\n",
    "    col2 = stops.First_2.unique()\n",
    "    col3 = stops.First_3.unique()\n",
    "    col4 = stops.Last_1.unique()\n",
    "    col5 = stops.Last_1.unique()\n",
    "    col6 = stops.Last_1.unique()\n",
    "    mysets = [col1, col2, col3, col4, col5, col6]\n",
    "    arbitrary_element = col1[0]\n",
    "    \n",
    "    full_set = set(frozenset().union(*mysets)) - arbitrary_element\n",
    "    \n",
    "    modified_frame = dataframe\n",
    "    refined_frame  = dataframe.loc[(dataframe.Stop_ID == arbitrary_element)]\n",
    "    for element in full_set - arbitrary_element:\n",
    "        next_frame = dataframe.loc[(dataframe.Stop_ID == element)]\n",
    "        \n",
    "        #todo: investigate if iteritive concat is faster or if concat a list of dfs is faster.\n",
    "        #aligning before concat is generally a speedup\n",
    "        #basis list.join is faster than iter string concat.\n",
    "        refined_frame, next_frame = refined_frame.align(next_frame, axis=1)\n",
    "    \n",
    "    return refined_frame\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# test_me = pd.read_csv(\"route4-reconstructed.csv\", index_col = None)\n",
    "# test_me = fix_LID_and_Dir(test_me)\n",
    "# test_me.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Construction 3.\n",
    "Drops all Not at stop data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Construction X.\n",
    "Takes identifies a single journey for a route and finds the nearest lat lng to the known missing stop. enter that as a row of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def find_nearest_stop():\n",
    "\n",
    "    for each missing bus stop in list of known stops. for each bus stop missing on a journey find the closest lat lng geometrically to known missing stop and add that to the dataframe. These will be the only at stop == 0 allowed in the frame. so they are identifiably and removeable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Construction 4.\n",
    "Takes both re-construction methods and implements them on all files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def re_construct(path, files, month, columns):\n",
    "\n",
    "    read = pd.read_csv\n",
    "    fix_jpid = wrap_JPID\n",
    "    fix_lidir = fix_LID_and_Dir\n",
    "    \n",
    "    for file in files:\n",
    "        print(\"Reconstructing\", file)\n",
    "        #reads csv from data folder in cwd    \n",
    "        #This line may need to be changed to use \"/\" instead of \"\\\\\" to run on mac or linux.\n",
    "        modify_me = read(path+\"\\\\\"+file, index_col=None, header=0, encoding=\"utf-8\", converters = {\"Journey_Pattern_ID\":str, \"LineID\":str, \"Direction\":str })\n",
    "        \n",
    "        #Sets columns names for dataframe (so it can operated easily and readably)\n",
    "        modify_me.columns = columns\n",
    "        modified_me = drop_columns(modify_me)\n",
    "        print(\"\\tDropped Columns)\n",
    "        \n",
    "        modify_me = drop_it_like_its_stop(modify_me)\n",
    "        print(\"\\tDropped Not Stop data\")\n",
    "        \n",
    "        #i'm the most expensive\n",
    "        modify_me = fix_jpid(modify_me)\n",
    "        print(\"\\tFixed JPIDs\")\n",
    "              \n",
    "        modify_me = fix_lidir(modify_me)\n",
    "        print(\"\\tDirection & LineID Re-Derived\")\n",
    "              \n",
    "        modify_me = modify_me.drop_duplicates(keep=\"first\")\n",
    "        print(\"\\tDropped Dupes\")\n",
    "        #stops made\n",
    "        #drop middle\n",
    "        \n",
    "        modify_me.to_csv(\"re_constructed_\"+month+\"\\\\re_con_\"+file, encoding = \"utf-8\", index=False)\n",
    "        print(\"\\tSaved!\")\n",
    "        \n",
    "        \n",
    "def wrap_re_construct(month):\n",
    "    path = os.getcwd()\n",
    "    path = path +\"\\\\\"+month+\"DayRawCopy\"\n",
    "    \n",
    "    #stores contents of a folder as a list.\n",
    "    contents = os.listdir(path)\n",
    "    \n",
    "    \n",
    "    columns   =    [\"Timestamp\",\n",
    "                    \"LineID\", \n",
    "                    \"Direction\",\n",
    "                    \"Journey_Pattern_ID\", \n",
    "                    \"Timeframe\", \n",
    "                    \"Vehicle_Journey_ID\", \n",
    "                    \"Operator\", \n",
    "                    \"Congestion\", \n",
    "                    \"Lon\",\n",
    "                    \"Lat\", \n",
    "                    \"Delay\", \n",
    "                    \"Block_ID\",\n",
    "                    \"Vehicle_ID\",\n",
    "                    \"Stop_ID\",\n",
    "                    \"At_Stop\"]\n",
    "    \n",
    "      \n",
    "        #Define load for each thread\n",
    "    #todo: use queue to improve (marginal)\n",
    "    total2 = len(contents)\n",
    "    iter1 = set(contents[:total2//2])\n",
    "    iter2 = set(contents[total2//2:])\n",
    "        \n",
    "        \n",
    "    #Instantiate Threads\n",
    "    thread1 = threading.Thread(target = re_construct, kwargs=dict(path=path, files=iter1, month=month, columns=columns))\n",
    "    thread2 = threading.Thread(target = re_construct, kwargs=dict(path=path, files=iter2, month=month, columns=columns))\n",
    "        \n",
    "        \n",
    "    #Start each in parallel\n",
    "    thread1.start()\n",
    "    thread2.start()\n",
    "        \n",
    "        \n",
    "    #threads wait for eachother to complete and merge.\n",
    "    thread1.join()\n",
    "    thread2.join()\n",
    "    \n",
    "    print(\"Multithreading Complete\")\n",
    "    return \"SUCCESS!\"\n",
    "\n",
    "\n",
    "#     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #this is the path to Dec data \n",
    "# path=os.getcwd() + \"\\\\data1\"\n",
    "\n",
    "# re_construct(path, \"Dec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #this is the path to november data \n",
    "# path=os.getcwd() + \"\\\\data1\"\n",
    "\n",
    "# #this will save to Nov file\n",
    "# re_construct(path, \"Nov\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #This is the path to the copied data\n",
    "%%time\n",
    "pool = Pool(processes=2) # initialize the Pool.\n",
    "months=[\"Jan\", \"Nov\"]\n",
    "result = pool.map(mp1_functions.wrap_re_construct, months)       # map f to the data using the Pool of processes to do the work \n",
    "pool.close() # No more processes\n",
    "pool.join()\n",
    "print(\"Result: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction Part 1.\n",
    "- This function is used to extract a single route from all files.\n",
    "- Requirement: the folder this file is contained in __MUST CONTAIN__ a folder called __\"data\"__ containing all the files where extraction will occur. \n",
    "\n",
    "\n",
    "- This will need to run for every LineID. It is not scalable to call it for every route, this must be automated. Within another function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def extract_route(path, route):\n",
    "    \n",
    "    #should help loop run a little faster.\n",
    "    read = pd.read_csv\n",
    "    concat = pd.concat\n",
    "    \n",
    "    #stores contents of a folder as a list.\n",
    "    contents = os.listdir(path)\n",
    "    \n",
    "    #small performance improvement.\n",
    "    content_length = len(contents)\n",
    "    \n",
    "    columns   =    [\"Timestamp\",\n",
    "                    \"LineID\", \n",
    "                    \"Direction\",\n",
    "                    \"Journey_Pattern_ID\", \n",
    "                    \"Timeframe\", \n",
    "                    \"Vehicle_Journey_ID\", \n",
    "                    \"Operator\", \n",
    "                    \"Congestion\", \n",
    "                    \"Lon\",\n",
    "                    \"Lat\", \n",
    "                    \"Delay\", \n",
    "                    \"Block_ID\",\n",
    "                    \"Vehicle_ID\",\n",
    "                    \"Stop_ID\",\n",
    "                    \"At_Stop\"]\n",
    "    \n",
    "    \n",
    "    #reads csv from data folder in cwd    \n",
    "    #This line may need to be changed to use \"/\" instead of \"\\\\\" to run on mac or linux.\n",
    "    accumulator = read(path+\"\\\\\"+contents[0], index_col=None, header=0, encoding=\"utf-8\")\n",
    "    \n",
    "    #Sets columns names for dataframe (so it can operated easily and readably)\n",
    "    accumulator.columns = columns\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(content_length):\n",
    "        print(\"extracting\", route, \"from file\", i)\n",
    "        #This line may need to be changed to use \"/\" instead of \"\\\\\" to run on mac or linux.\n",
    "        next_df = read(path+\"\\\\\"+contents[i], index_col=None, header=0)\n",
    "        \n",
    "        accumulator, next_df = accumulator.align(next_df, axis=1)\n",
    "\n",
    "        #Line Continuation char is used for readability\n",
    "        accumulator = concat([accumulator[( accumulator[\"LineID\"] == route)], \\\n",
    "                                 next_df [(   next_df[\"LineID\"]   == route)]] \\\n",
    "                                 , axis=0)\n",
    "        \n",
    "#         print(accumulator.shape, \"acc\") (use this to track concats are happening correctly; debugging)\n",
    "        \n",
    "    return accumulator\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # the first line takes the path to the folder containing this document.\n",
    "# # for me this path looks like this. C:\\Users\\Andy\\Desktop\\DataAnalytics\n",
    "# # then \\data is appended to the path. (not it is the path to the data folder)\n",
    "# # on a windows system the seperators for paths is \"\\\" , on mac or linux  us /\n",
    "# # I also use two seperators as the \"\\\" is the esc char in python, so to use it in the path it must be escaped from itself.\n",
    "\n",
    "# #if you change the folder name or need to make this run on mac or linux, this is the line you change.\n",
    "# path=os.getcwd() + \"\\\\data1\"\n",
    "# #calls function\n",
    "# df_1 = extract_route(path, \"13\") #this would extract for route 4.\n",
    "\n",
    "# #shows \"shape of result\", important for debugging.\n",
    "# df_1.shape\n",
    "# df_1.to_csv(\"route-13-raw1.csv\" , encoding=\"utf-8\")\n",
    "\n",
    "# path=os.getcwd() + \"\\\\data2\"\n",
    "\n",
    "# df_1 = extract_route(path, \"26\", encoding = \"utf-8\" )\n",
    "# df_1.to_csv( \"route-26-raw2.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction Part 2.\n",
    "This function takes the first function (which extracts a single route from files) and scales it it.\n",
    "We don't have a complete list of routes, So we will skim over the files as quickly as we can and put together a list of routes.\n",
    "\n",
    "It would be more efficient to construct this complete list of routes as we perform the other functions, but to save programmer time, I will develop this independently, and run it once, and save the result. This saves time if I need to do partial test runs of the other extract functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def find_routes(path):\n",
    "    #should help loop run a little faster.\n",
    "    read = pd.read_csv\n",
    "    \n",
    "    #stores contents of a folder as a list.\n",
    "    contents = os.listdir(path)\n",
    "    \n",
    "    #small performance improvement.\n",
    "    content_length = len(contents)\n",
    "    \n",
    "    columns   =    [\"Timestamp\",\n",
    "                    \"LineID\", \n",
    "                    \"Direction\",\n",
    "                    \"Journey_Pattern_ID\", \n",
    "                    \"Timeframe\", \n",
    "                    \"Vehicle_Journey_ID\", \n",
    "                    \"Operator\", \n",
    "                    \"Congestion\", \n",
    "                    \"Lon\",\n",
    "                    \"Lat\", \n",
    "                    \"Delay\", \n",
    "                    \"Block_ID\",\n",
    "                    \"Vehicle_ID\",\n",
    "                    \"Stop_ID\",\n",
    "                    \"At_Stop\"]\n",
    "    \n",
    "    \n",
    "    #reads csv from data folder in cwd    \n",
    "    #This line may need to be changed to use \"/\" instead of \"\\\\\" to run on mac or linux.\n",
    "    accumulator = read(path+\"\\\\\"+contents[0], index_col=None, header=0, encoding=\"utf-8\")\n",
    "    \n",
    "    #Sets columns names for dataframe (so it can operated easily and readably)\n",
    "    accumulator.columns = columns\n",
    "    \n",
    "    set_of_routes = set(accumulator.LineID.unique())\n",
    "#     comparison_set = set_of_routes #this is used to test the necesity of looping through all files\n",
    "    \n",
    "    #Should help loop run a bit faster\n",
    "    unite = set_of_routes.union\n",
    "    \n",
    "    #read through all files once and determine maximum set of routes\n",
    "    for i in range(1,content_length):\n",
    "        \n",
    "        #if you change the folder name or need to make this run on mac or linux, this is the line you change.\n",
    "        next_df = read(path+\"\\\\\"+contents[i], index_col=None, header=0, encoding =\"utf-8\")\n",
    "        next_df.columns = columns\n",
    "        \n",
    "        next_set_of_routes = set(next_df.LineID.unique())\n",
    "        set_of_routes = unite(next_set_of_routes)\n",
    "        print(\"Constructing Route Set... Current File:\", i)\n",
    "    \n",
    "    # This block of code can be used to show that extracting all routes from a single file does not work, its nexesary to go through each file exhaustively\n",
    "#\n",
    "#     print(\"total set of routes\\n\", set(set_of_routes)==set(comparison_set))\n",
    "#     print(comparison_set - set_of_routes)\n",
    "#     print(set_of_routes - comparison_set)\n",
    "    \n",
    "    intset = {str(x) for x in set_of_routes if isinstance(x, int)}\n",
    "    strset = {x for x in set_of_routes if isinstance(x, str)}\n",
    "    set_of_routes = sorted(intset.union(strset), key=str)\n",
    "    \n",
    "    return set_of_routes\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #if you change the folder name or need to make this run on mac or linux, this is the line you change.\n",
    "# path=os.getcwd() + \"\\\\data1\"\n",
    "\n",
    "# all_routes = find_routes(path)\n",
    "\n",
    "# print(all_routes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction Part 3.\n",
    "This function will take all sets of routes from re-constructed files and then for each route it will scrape the all re-constrcted files (which are sorted by date) into files which are dedicated to a single route.\n",
    "\n",
    "at this stage route should be passed in in the format 0004, for the number 4 bus. The path is to the re-constructed folder for a particular month.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def complete_extraction(path):\n",
    "    \n",
    "    all_routes = find_routes(path)\n",
    "    \n",
    "    for route in all_routes:\n",
    "        dataframe = extract_route(path, route)\n",
    "        dataframe.to_csv(\"alpha-\"+route+\".csv\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #if you change the folder name or need to make this run on mac or linux, this is the line you change.\n",
    "path = os.getcwd() + \"\\\\data1\"\n",
    "\n",
    "complete_extraction(path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I Cut some code used for experimentation \n",
    "(it can be seen in the previous commit), know that we have explored the data many of these properties are known and documented.) I have a spare copy of the code that inhabbited this section, but excluded it from this document as it was not clean and clear cut and may only serve to disorientate the reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import time\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import threading\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def fix_JPID(vjids, day, dataframe):\n",
    "    loc = dataframe.loc\n",
    "    total = len(vjids)\n",
    "    current = 0\n",
    "    for run in vjids:\n",
    "        current+=100\n",
    "        print(current//total,\"% Reconstructed for \", day)\n",
    "        vids = set(dataframe[ (dataframe    [ \"Timeframe\" ]     ==  day   ) &\\\n",
    "                          (dataframe [\"Journey_Pattern_ID\"] == \"null\" ) &\\\n",
    "                          (dataframe [\"Vehicle_Journey_ID\"] ==  run   ) ]\\\n",
    "                          .Vehicle_ID.unique())\n",
    "\n",
    "        for vehicle in vids:\n",
    "            re_construct = list(loc[ (dataframe    [\"Timeframe\"]       ==   day )  &\\\n",
    "                                     (dataframe [\"Vehicle_Journey_ID\"] ==   run )  &\\\n",
    "                                     (dataframe    [\"Vehicle_ID\"]      == vehicle ),\\\n",
    "                                     \"Journey_Pattern_ID\" ].unique() )  \n",
    "\n",
    "            if len(re_construct) == 2:\n",
    "                if re_construct[0] != \"null\":\n",
    "                    loc[ (dataframe    [\"Timeframe\"]       ==  day    ) &\\\n",
    "                         (dataframe [\"Vehicle_Journey_ID\"] ==  run    ) &\\\n",
    "                         (dataframe    [\"Vehicle_ID\"]      == vehicle ), \\\n",
    "                         \"Journey_Pattern_ID\" ] = re_construct[0]\n",
    "\n",
    "                else:\n",
    "                    loc[ (dataframe    [\"Timeframe\"]        == day     ) &\\\n",
    "                         (dataframe [\"Vehicle_Journey_ID\"]  == run     ) &\\\n",
    "                         (dataframe    [\"Vehicle_ID\"]       == vehicle ), \\\n",
    "                         \"Journey_Pattern_ID\" ] = re_construct[1]\n",
    "                    \n",
    "    return dataframe\n",
    "                \n",
    "\n",
    "\n",
    "    \n",
    "def wrap_JPID(dataframe):\n",
    "    #every day\n",
    "    time_frames = set(dataframe[ dataframe[\"Journey_Pattern_ID\"] == \"null\" ].Timeframe.unique())\n",
    "    total1=len(time_frames)\n",
    "    \n",
    "    for day in time_frames:\n",
    "        vjid = list( dataframe[ (dataframe   [ \"Timeframe\" ]      ==  day  ) &\\\n",
    "                                (dataframe [\"Journey_Pattern_ID\"] == \"null\") ]\\\n",
    "                                .Vehicle_Journey_ID.unique())                     \n",
    "        \n",
    "    return fix_JPID(vjid, day, dataframe)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fix_LID_and_Dir(dataframe):\n",
    "    modified_frame = dataframe\n",
    "    modified_frame[\"Journey_Pattern_ID\"] = dataframe[\"Journey_Pattern_ID\"].astype(\"str\")\n",
    "    modified_frame[\"LineID\"] =  dataframe[\"Journey_Pattern_ID\"].str[:4]\n",
    "    modified_frame[\"Direction\"] = dataframe[\"Journey_Pattern_ID\"].str[4]\n",
    "    \n",
    "    return modified_frame\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def drop_columns(dataframe):\n",
    "    modified_frame = dataframe\n",
    "    modified_frame = modified_frame.drop(\"Congestion\", axis=1)\n",
    "    modified_frame = modified_frame.drop(\"Delay\", axis=1)\n",
    "    modified_frame = modified_frame.drop(\"Block_ID\", axis=1)\n",
    "    modified_frame = modified_frame.drop(\"Operator\", axis=1)\n",
    "    \n",
    "    return modified_frame\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def drop_it_like_its_stop(dataframe):\n",
    "    modified_frame = dataframe.loc[(dataframe.At_Stop == 1)]\n",
    "\n",
    "    return modified_frame\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def drop_null_JPIDS(dataframe):\n",
    "\n",
    "    return dataframe[dataframe[\"Journey_Pattern_ID\"] != \"null\" ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def remove_idle_at_stop(df):\n",
    "    df = df.drop_duplicates(subset='Stop_ID', keep='first')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def stops_made(df):\n",
    "    for i in range(df['Timestamp'].size):\n",
    "        df['Stops_Made'].values[i] = i+1\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def make_stops_made(dataframe):\n",
    "    modified_frame = dataframe\n",
    "    modified_frame[\"Stops_Made\"] = 0\n",
    "    modified_frame = modified_frame.groupby(['Vehicle_Journey_ID', 'Timeframe'])\n",
    "    modified_frame = modified_frame.apply(remove_idle_at_stop)\n",
    "    modified_frame = modified_frame.groupby(['Vehicle_Journey_ID', 'Timeframe'])\n",
    "    modified_frame = modified_frame.apply(stops_made)\n",
    "    \n",
    "    return modified_frame    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def drop_middle(dataframe):\n",
    "    concat = pd.concat\n",
    "    stops = pd.read_csv(\"end_stops.csv\", encoding=\"utf-8\", converters={\"First_1\":str, \"First_2\":str, \"First_3\":str, \"Last_1\":str, \"Last_2\":str, \"Last_3\":str})\n",
    "    col1 = stops.First_1.unique()\n",
    "    col2 = stops.First_2.unique()\n",
    "    col3 = stops.First_3.unique()\n",
    "    col4 = stops.Last_1.unique()\n",
    "    col5 = stops.Last_2.unique()\n",
    "    col6 = stops.Last_3.unique()\n",
    "    mysets = [col1, col2, col3, col4, col5, col6]\n",
    "    arbitrary_element = col1[0]\n",
    "    \n",
    "    full_set = set(frozenset().union(*mysets))\n",
    "    \n",
    "    \n",
    "    numset = {str(x) for x in full_set if ( isinstance(x, int) or isinstance(x, float) ) }\n",
    "    strset = {x for x in full_set if isinstance(x, str)}\n",
    "    set_of_routes = sorted(numset.union(strset), key=str)\n",
    "    \n",
    "    modified_frame = dataframe\n",
    "    refined_frame  = dataframe.loc[(dataframe.Stop_ID == arbitrary_element)]\n",
    "    for element in full_set:\n",
    "        next_frame = dataframe.loc[(dataframe.Stop_ID == element)]\n",
    "        \n",
    "        refined_frame, next_frame = refined_frame.align(next_frame, axis=1)\n",
    "        \n",
    "        refined_frame = concat([refined_frame, next_frame], axis = 0)\n",
    "    \n",
    "    return refined_frame\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def re_construct(path, files, month, columns):\n",
    "    read = pd.read_csv\n",
    "    fix_jpid = wrap_JPID\n",
    "    fix_lidir = fix_LID_and_Dir\n",
    "    \n",
    "    for file in files:\n",
    "        print(\"Reconstructing\", file)\n",
    "        modify_me = read(path+\"\\\\\"+file, index_col=None, header=0, encoding=\"utf-8\", converters = {\"Journey_Pattern_ID\":str, \"LineID\":str, \"Direction\":str, \"Stop_ID\":str })\n",
    "        \n",
    "        modify_me.columns = columns\n",
    "        modify_me = drop_columns(modify_me)\n",
    "        print(\"\\tDropped Columns\")\n",
    "        \n",
    "        modify_me = drop_it_like_its_stop(modify_me)\n",
    "        print(\"\\tDropped Not Stop data\")\n",
    "        \n",
    "#         modify_me = fix_jpid(modify_me)\n",
    "        modify_me = drop_null_JPIDS(modify_me)\n",
    "#         print(\"\\tFixed JPIDs\")\n",
    "              \n",
    "        modify_me = fix_lidir(modify_me)\n",
    "        print(\"\\tDirection & LineID Re-Derived\")\n",
    "    \n",
    "        \n",
    "        modify_me = make_stops_made(modify_me)\n",
    "        print(\"\\t idling removed and stops_made column created\")\n",
    "        \n",
    "        modify_me = modify_me.drop_duplicates(keep=\"first\")\n",
    "        print(\"\\tDropped Dupes\")\n",
    "        \n",
    "        modify_me = drop_middle(modify_me)\n",
    "        print(\"Dropped Middle of each journey\")\n",
    "        \n",
    "        modify_me.to_csv(\"re_constructed_\"+month+\"\\\\re_con_\"+file, encoding = \"utf-8\", index=False)\n",
    "        print(\"\\tSaved!\")\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "def wrap_re_construct(month):\n",
    "    path = os.getcwd()\n",
    "    path = path +\"\\\\\"+month+\"DayRawCopy\"\n",
    "    contents = os.listdir(path)\n",
    "    \n",
    "    columns   =    [\"Timestamp\",\n",
    "                    \"LineID\", \n",
    "                    \"Direction\",\n",
    "                    \"Journey_Pattern_ID\", \n",
    "                    \"Timeframe\", \n",
    "                    \"Vehicle_Journey_ID\", \n",
    "                    \"Operator\", \n",
    "                    \"Congestion\", \n",
    "                    \"Lon\",\n",
    "                    \"Lat\", \n",
    "                    \"Delay\", \n",
    "                    \"Block_ID\",\n",
    "                    \"Vehicle_ID\",\n",
    "                    \"Stop_ID\",\n",
    "                    \"At_Stop\"]\n",
    "    \n",
    "    total2 = len(contents)\n",
    "    iter1 = set(contents[:total2//2])\n",
    "    iter2 = set(contents[total2//2:])\n",
    "        \n",
    "    thread1 = threading.Thread(target = re_construct, kwargs=dict(path=path, files=iter1, month=month, columns=columns))\n",
    "    thread2 = threading.Thread(target = re_construct, kwargs=dict(path=path, files=iter2, month=month, columns=columns))\n",
    "        \n",
    "    thread1.start()\n",
    "    thread2.start()\n",
    "        \n",
    "    thread1.join()\n",
    "    thread2.join()\n",
    "    \n",
    "    print(\"Multithreading Complete\")\n",
    "    return \"SUCCESS!\"\n",
    "              \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def extract_route(path, route):\n",
    "    read = pd.read_csv\n",
    "    concat = pd.concat\n",
    "    contents = os.listdir(path)\n",
    "    content_length = len(contents)\n",
    "   \n",
    "    accumulator = read(path+\"\\\\\"+contents[0], index_col=None, header=0, encoding=\"utf-8\")\n",
    "    \n",
    "    for i in range(content_length):\n",
    "        print(\"extracting\", route, \"from file\", i)\n",
    "        next_df = read(path+\"\\\\\"+contents[i], index_col=None, header=0)\n",
    "        accumulator, next_df = accumulator.align(next_df, axis=1)\n",
    "        accumulator = concat([accumulator[( accumulator[\"LineID\"] == route)], \\\n",
    "                                 next_df [(   next_df[\"LineID\"]   == route)]] \\\n",
    "                                 , axis=0)\n",
    "        \n",
    "    return accumulator\n",
    "              \n",
    "\n",
    "    \n",
    "    \n",
    "def find_routes(path):\n",
    "    read = pd.read_csv\n",
    "    contents = os.listdir(path)\n",
    "    content_length = len(contents)\n",
    "    \n",
    "    accumulator = read(path+\"\\\\\"+contents[0], index_col=None, header=0, encoding=\"utf-8\")\n",
    "    \n",
    "    set_of_routes = set(accumulator.LineID.unique())\n",
    "    unite = set_of_routes.union\n",
    "    \n",
    "    for i in range(1,content_length):\n",
    "        next_df = read(path+\"\\\\\"+contents[i], index_col=None, header=0, encoding =\"utf-8\")\n",
    "        next_set_of_routes = set(next_df.LineID.unique())\n",
    "        set_of_routes = unite(next_set_of_routes)\n",
    "        print(\"Constructing Route Set... Current File:\", i)\n",
    "    \n",
    "    numset = {str(x) for x in set_of_routes if ( isinstance(x, int) or isinstance(x, float) ) }\n",
    "    strset = {x for x in set_of_routes if isinstance(x, str)}\n",
    "    set_of_routes = sorted(numset.union(strset), key=str)\n",
    "    \n",
    "    return set_of_routes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def complete_extraction(month):\n",
    "    path = os.getcwd() + \"\\\\re_constructed_\" + month\n",
    "    all_routes = find_routes(path)\n",
    "    \n",
    "    for route in all_routes:\n",
    "        dataframe = extract_route(path, route)\n",
    "        dataframe.to_csv(month+\"_routes\\\\alpha-\"+route+\".csv\")\n",
    "\n",
    "    return \"Success\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
