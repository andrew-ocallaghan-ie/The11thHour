{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/reggiemurphy/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# The ultimate target feature: time from one stop to another\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, datetime\n",
    "from patsy import dmatrices\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import export_graphviz, DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.externals import joblib \n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Read csv file into a dataframe.\n",
    "df = pd.read_csv('csv_data/route4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Data Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Rename column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.rename(columns={'Timeframe': 'Start_date'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Dropping duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=df.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Dropping constant columns or columns with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop('Direction', axis=1)\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "df = df.drop('Congestion', axis=1)\n",
    "df[df.Journey_Pattern_ID == 'null']\n",
    "df = df[df['Journey_Pattern_ID'] != '00040002']\n",
    "df = df[df['Journey_Pattern_ID'] != '00041002']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Remove rows where bus is not at stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.loc[(df != 0).all(axis=1), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Group to normalise time & remove rows where bus idle at stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create empty column which will hold normalised time\n",
    "df['normal_time'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create empty column which will hold the stop order\n",
    "df['stop_order'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grouped_df = df.groupby(['Vehicle_Journey_ID', 'Start_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_time(df):\n",
    "    \"\"\"Normalise the time for each journey\"\"\"\n",
    "    for i in range(df['Timestamp'].size):\n",
    "        df['normal_time'].values[i] = (df['Timestamp'].values[i] - df['Timestamp'].values[0]) / 1000000\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm_gb = grouped_df.apply(normalize_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grouped_df = norm_gb.groupby(['Vehicle_Journey_ID', 'Start_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_idle_at_stop(df):\n",
    "    df = df.drop_duplicates(subset='Stop_ID', keep='first')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm_gb = grouped_df.apply(remove_idle_at_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/reggiemurphy/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:1: FutureWarning: 'Vehicle_Journey_ID' is both a column name and an index level.\n",
      "Defaulting to column but this will raise an ambiguity error in a future version\n",
      "  if __name__ == '__main__':\n",
      "/Users/reggiemurphy/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:1: FutureWarning: 'Start_date' is both a column name and an index level.\n",
      "Defaulting to column but this will raise an ambiguity error in a future version\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "grouped_df = norm_gb.groupby(['Vehicle_Journey_ID', 'Start_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def route_order(df):\n",
    "    for i in range(df['Timestamp'].size):\n",
    "        df['stop_order'].values[i] = i\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = grouped_df.apply(route_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grouped_df = norm_gb.groupby(['Vehicle_Journey_ID', 'Start_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grouped_df.get_group((4601, '2013-01-29'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Add new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"Time\"] = pd.to_datetime(df['Timestamp']*1000, unit=\"ns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['IsWeekend'] = np.where((df['Time'].dt.dayofweek > 5), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['HourOfDay'] = df['Time'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['IsEveningTime'] = np.where((df['Time'].dt.hour > 17), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['IsPeakTime'] = np.where(((df['HourOfDay'] >= 7) & (df['HourOfDay'] <= 10)) | ((df['HourOfDay'] >= 16) & (df['HourOfDay'] <= 19)), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['SchoolHoliday'] = np.where((df['Time'].dt.date == datetime(2012, 11, 1).date()) | (\n",
    "\n",
    "df['Time'].dt.date == datetime(2012, 11, 2).date()) | (\n",
    "\n",
    "                                       df['Time'].dt.date == datetime(2013, 1, 1).date()) | (\n",
    "\n",
    "                                       df['Time'].dt.date == datetime(2013, 1, 2).date()) | (\n",
    "\n",
    "                                       df['Time'].dt.date == datetime(2013, 1, 3).date()) | (\n",
    "\n",
    "                                       df['Time'].dt.date == datetime(2013, 1, 4).date()), 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Merge Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Merge bus stop info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df=df.drop_duplicates(subset='Time', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_bus_stop = pd.read_csv('csv_data/busstopinfo.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_bus_stop = df_bus_stop.rename(columns={'stopid': 'Stop_ID'})\n",
    "df_bus_stop = df_bus_stop.rename(columns={'fullname': 'Stop_name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_bus_stop = df_bus_stop[['Stop_ID', 'Stop_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.merge(df, df_bus_stop, on=['Stop_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_bus_stop_route4 = df_bus_stop.loc[(df_bus_stop.route1 == '4')]\n",
    "# df_bus_stop_route4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_southbound = df.loc[(df.Vehicle_ID == 43048) & (df.Vehicle_Journey_ID == 4909)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df[(df.Vehicle_ID == 43048)][\"Vehicle_Journey_ID\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get northbound order of stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_northbound = df.loc[(df.Vehicle_ID == 43048) & (df.Vehicle_Journey_ID == 4572)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_bus_stop_route4 = df_bus_stop_route4[['Stop_ID', 'fullname']]\n",
    "# df_bus_stop_route4.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_south = df_southbound.drop_duplicates(subset='Stop_ID', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_south['stop_order'] = df_south.groupby(['Vehicle_Journey_ID', 'At_Stop', 'Start_date', 'Vehicle_ID'])['Timestamp','Stop_ID'].cumcount().reindex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_north = df_northbound.drop_duplicates(subset='Stop_ID', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_north['stop_order'] = df_north.groupby(['Vehicle_Journey_ID', 'At_Stop', 'Start_date', 'Vehicle_ID'])['Timestamp','Stop_ID'].cumcount().reindex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_order = df_south[['Stop_ID', 'stop_order']]\n",
    "# df_order = df_order.drop_duplicates(subset='Stop_ID', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_order2 = df_north[['Stop_ID', 'stop_order']]\n",
    "# df_order2=df_order2.drop_duplicates(subset='Stop_ID', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# frames = [df_order, df_order2]\n",
    "# df_order3 = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_bus_stop_route4= pd.merge(df_order3, df_bus_stop_route4, on=['Stop_ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Merge weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_weather = pd.read_csv('csv_data/weather_data.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_weather = df_weather.rename(columns={'Rainfall (Yes or No)': 'Rain'})\n",
    "df_weather = df_weather.rename(columns={'Temperature C': 'Temperature'})\n",
    "df_weather = df_weather.rename(columns={'Relative Humidity (%)': 'Humidity'})\n",
    "df_weather = df_weather.rename(columns={'Over 1mm Rain?': 'Heavy_rain'})\n",
    "df_weather = df_weather.rename(columns={'Precipitation (mm)': 'Precipitation'})\n",
    "df_weather = df_weather.rename(columns={'Date': 'Time'})\n",
    "df_weather['Time'] = pd.to_datetime(df_weather['Time'])\n",
    "df_weather.sort_values(['Time'], ascending=[True], inplace=True)\n",
    "df_weather['HourOfDay'] = df_weather['Time'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.sort_values(['Time'], ascending=[True], inplace=True)\n",
    "df_weather.sort_values(['Time'], ascending=[True], inplace=True)\n",
    "df =  pd.merge_asof(df, df_weather, on='Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Timestamp', 'LineID', 'Journey_Pattern_ID', 'Start_date',\n",
       "       'Vehicle_Journey_ID', 'Operator', 'Lon', 'Lat', 'Delay', 'Block_ID',\n",
       "       'Vehicle_ID', 'Stop_ID', 'At_Stop', 'normal_time', 'stop_order', 'Time',\n",
       "       'IsWeekend', 'HourOfDay_x', 'IsEveningTime', 'IsPeakTime',\n",
       "       'SchoolHoliday', 'Stop_name', 'Precipitation', 'Temperature',\n",
       "       'Vapour Pressure (hPa)                ', 'Humidity',\n",
       "       'Mean Sea Level Pressure (hPa)', 'Rain', 'Heavy_rain'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#code from here: https://stackoverflow.com/questions/27313647/merging-two-pandas-dataframes-results-in-duplicate-columns\n",
    "\n",
    "def drop_y(df):\n",
    "    # list comprehension of the cols that end with '_y'\n",
    "    to_drop = [x for x in df if x.endswith('_y')]\n",
    "    df.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "drop_y(df)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rename_x(df):\n",
    "    for col in df:\n",
    "        if col.endswith('_x'):\n",
    "            df.rename(columns={col:col.rstrip('_x')}, inplace=True)\n",
    "rename_x(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Remove and categories columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Drop missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[df['Journey_Pattern_ID'] != 'null']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Categorise to continuous and categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['IsWeekend'] = df['IsWeekend'].astype('category')\n",
    "df['HourOfDay'] = df['HourOfDay'].astype('category')\n",
    "df['IsEveningTime'] = df['IsEveningTime'].astype('category')\n",
    "df['SchoolHoliday'] = df['SchoolHoliday'].astype('category')\n",
    "df['IsPeakTime'] = df['IsPeakTime'].astype('category')\n",
    "df['Operator'] = df['Operator'].astype('category')\n",
    "df['At_Stop'] = df['At_Stop'].astype('category')\n",
    "categorical_columns = df.select_dtypes(['category']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "continuous_columns = df.select_dtypes(['int64', 'float64']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Drop columns no longer needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop('Lat', axis=1)\n",
    "df = df.drop('Lon', axis=1)\n",
    "df = df.drop('Block_ID', axis=1)\n",
    "df = df.drop('Operator', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Create time to destination feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['end_time'] = df.groupby(['stop_order','Journey_Pattern_ID', 'Start_date', 'HourOfDay'])['Timestamp'].transform(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['time_diff_to_destination'] = df['end_time'] - df['Timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['distance_to_end'] = df.stop_order.max() - df.stop_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Timestamp', 'LineID', 'Journey_Pattern_ID', 'Start_date',\n",
       "       'Vehicle_Journey_ID', 'Delay', 'Vehicle_ID', 'Stop_ID', 'At_Stop',\n",
       "       'normal_time', 'stop_order', 'Time', 'IsWeekend', 'HourOfDay',\n",
       "       'IsEveningTime', 'IsPeakTime', 'SchoolHoliday', 'Stop_name',\n",
       "       'Precipitation', 'Temperature', 'Vapour Pressure (hPa)                ',\n",
       "       'Humidity', 'Mean Sea Level Pressure (hPa)', 'Rain', 'Heavy_rain',\n",
       "       'end_time', 'time_diff_to_destination', 'distance_to_end'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Save DF to be used in Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save cleaned dataframe to new CSV file\n",
    "df.to_csv('csv_data/bus_route4_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_train = df[:130000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_test = df[130000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_group = df.groupby(['Vehicle_Journey_ID', 'Start_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for k, g in df_group:\n",
    "#     model = ols('time_diff_to_destination ~ distance_to_end + Rain + Temperature +Humidity + IsWeekend +HourOfDay + SchoolHoliday', g)\n",
    "#     results = model.fit()\n",
    "#     print(results.summary())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
